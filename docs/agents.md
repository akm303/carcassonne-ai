# Agents
## Table of Contents
1. Agent Types
2. Agent Design & Implementation


---
## 1. Agent Types
For our project we implemented several agents that can be used to play the game:
- Stochastic Agent          [`(random_agent.py)`](../agents/random_agent.py)
- MCTS Agent                [`(mcts_agent.py)`](../agents/mcts_agent.py)
- Q-learning Agent          [`(qlearn_agent.py)`](../agents/qlearn_agent.py)
- Sarsa Agent               [`(sarsa_agent.py)`](../agents/sarsa_agent.py)
- Sarsa($\lambda$) Agent    [`(sarsa_lambda_agent.py)`](../agent/sarsa_lambda_agent.py`)

Our agents fall into two categories:
- Non-Learning Agents (ie. Random and MCTS agents)
- Reinforcement Learning (RL) Agents (ie. Q-learning, Sarsa, and Sarsa($\lambda$) agents)



---
## 2. Agent Design & Implementation
Agent design inspired by Berkeley CS188 Pacman AI Project's Agent implementation ([course page](https://inst.eecs.berkeley.edu/~cs188/fa25/)).

An Agent will:
- observe the game state (ie. take the game state as input)
- select from a set of legal actions (action pairs):
    1. current tile rotation and placement coordinates
    2. optional meeple placement on meeple-less feature of current tile

We first define an abstract agent. Any subclass will need to implement a `getAction()` method which will choose an action based on the current `game.state`.

From there, we implemented the following agents:
- Random Agent
- MCTS Agent
- Q-Learning Agent
- Sarsa Agent
- Sarsa($\lambda$) Agent

---
## Non-Learning Agents
### Random Agent
The Random/Stochastic Agent selects action from all legal moves at random.

### MCTS Agent
At each round of play, this agent generates a tree that it uses to inform its action selection.
To select a move, it loops:
- generates a successor state based on a potential action
- simulates the rest of the game by randomly selecting moves (rollout)
- backpropogates results up the tree

After a fixed number of ndoes are added to the tree, the action with the highest potential score is selected.  
After the next player takes their turn, this agent shifts its root to the appropriate next node (based on the  
action it selected) and continues generating the tree, generating a fixed number of nodes each turn.

<!-- There are a number of potential optimizations that could be made -->


## RL Agents
Reinforcement learning agents use model-agnostic Q-tables (loaded from the `agents/params/` directory)
- Q-tables are model agnostic, in that, currently, they can be generated by any RL model then used by any other RL model.  
  For example, a Q-table may be generated by training a Q-learning model, but this Q-table can also be loaded into a Sarsa agent when playing a game.  
  The reverse is also true.
- To generate a Q-table, user may select a model, adversary, and number of iterations to run a game. More details in [training.md](training.md)
- All our premade models are trained against a stochastic adversary (ie. a random agent).

### Q-Learning Agent
Over multiple games, this agent generates a table of Q-values for each (state,action) pair using an epsilon-greedy exploration strategy.  
These agents learn based on evaluating the "optimal" actions of future states rather than actual actions taken (ie. off-policy)

### Sarsa Agent
Similar in design/impelemntation to Q-learning agent, except these agents evaluate based on the actual actions taken from a state (ie. on-policy).

### Sarsa($\lambda$) Agent
Similar in design/implementation to Sarsa agent, except these agents use a trace to affect expected values of different actions


