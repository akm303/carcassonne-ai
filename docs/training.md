RL = reinforcement learning
MCTS = monte-carlo tree search

# Training RL Models
RL agents (Q-Learning, Sarsa, and Sarsa($\lambda$)) use tables of Q-values to inform their action selection process.  
Technically, RL agents can play games as-is, since they generate Q-tables as they go.
They then iteratively improve as they play, adjusting Q-values as they go.

To speed up training, we created a `train.py` script, which can be invoked using the following format:
```sh
python train.py -i ITERATIONS -m MODEL [-a ADVERSARY] [-u uid]
# int (required*) {ITERATIONS}: number of games over which to train
# str (required*) {MODEL}: agent type to train (trainee agent)
#                 {MODEL} must be from set {'qlearning', 'sarsa', 'sarsalambda'} 
#                         (ie. from RL models)
# str (optional)  {ADVERSARY}: agent type to train against (adversary agent)
#                 {ADVERSARY} must be from set {'random', 'mcts'} 
#                             (ie. from non-learning models)
#                  default adversary is random agent
#                  * Note its possible to train with MCTS, but MCTS operation is slow 
# str (optional)  {UID}: unique id/name string of trainee agent
```

Examples:  
To train a Sarsa agent for 30 iterations against an MCTS agent:
```sh
python train.py -m sarsa -i 30 -a mcts
```

To train a Qlearning agent '1' for 400 iterations against a stochastic (random) agent:
```sh
python train.py -m qlearn -i 400 -u 1
```

The training process initializes a game between a valid trainee (RL model) and adversary (non-learning model)  
and runs multiple complete games between the two agents. Each complete game is called an episode.  
As each episode progresses, an RL agent adjusts its Q-table values (see [`agents.md`](agents.md) for more details).  
At the end of each episode, the Q-table is persisted to a file in the `agents/params/` directory.


Parameters
- Q-tables are model-agnostic. That is, a Q-table can currently be generated by any RL model, then used by any other RL model.
    For example, a Q-table may be generated by training a Q-learning model, but that Q-table can also be loaded into a Sarsa agent. The reverse is also true.
- All our premade models are trained against a stochastic adversary (ie. random agent)


---

## Design Choices to Note
### 1. Training against RL-agents  
Though technically feasible to train RL agents against any other type of agent (including other reinforcement learning agents),  
it became tricky to specify a particular parameter file for a trainer RL-agent to use (ie. to assign a particular set of Q-values to the trainer)  

Therefore, we decided to restrict adversaries to non-learning agents. 

#### Potential solutions:
- train an RL model to generate a 'trainer'/'adversary' Q-table, then hardcode the parameter file 
- allow training against an empty RL model, maybe train two RL models simultaneously
either way, will likely need to better organize input parameters and standardize naming convention better
